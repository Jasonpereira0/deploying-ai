{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully! Length: 51452 characters\n",
      "First 500 characters:\n",
      "www.hbr.org\n",
      "B\n",
      " \n",
      "EST  \n",
      " \n",
      "OF  HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "by Peter F . Drucker\n",
      " \n",
      "•\n",
      " \n",
      "Included with this full-text \n",
      " \n",
      "Harvard Business Review\n",
      " \n",
      " article:\n",
      "The Idea in Brief—the core idea\n",
      "The Idea in Practice—putting the idea to work\n",
      " \n",
      "1\n",
      " \n",
      "Article Summary\n",
      " \n",
      "2\n",
      " \n",
      "Managing Oneself\n",
      "A list of related materials, with annotations to guide further\n",
      "exploration of the article’s ideas and applications\n",
      " \n",
      "12\n",
      " \n",
      "Further Reading\n",
      "Success in the knowledge \n",
      "economy comes to those who \n",
      "know themselves—their \n",
      "strengths\n"
     ]
    }
   ],
   "source": [
    "# Import the PyPDFLoader from langchain_community to load PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader # This loader extracts text from each page of a PDF file\n",
    "import os\n",
    "\n",
    "document_folder = \"../05_src/documents/\"\n",
    "\n",
    "# I'm using the Peter Drucker article \"Managing Oneself\"\n",
    "pdf_filename = \"managing_oneself_drucker.pdf\"\n",
    "\n",
    "# Join the folder path and filename to create the complete file path\n",
    "file_path = os.path.join(document_folder, pdf_filename)\n",
    "\n",
    "# Note: Using a web loader instead to load the PDF which is a web link\n",
    "import requests # Import requests library to download content from the web\n",
    "\n",
    "pdf_url = \"https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf\"\n",
    "\n",
    "response = requests.get(pdf_url) # Make an HTTP GET request to download the PDF content\n",
    "\n",
    "temp_pdf_path = \"temp_article.pdf\" # Saving the PDF temporarily locally so it can load with PyPDFLoader\n",
    "\n",
    "with open(temp_pdf_path, 'wb') as f: # Open a file in binary write mode and save the downloaded content\n",
    "    f.write(response.content) # Write the binary content of the PDF to the file\n",
    "\n",
    "# Create a PyPDFLoader variable with the path to the PDF file\n",
    "loader = PyPDFLoader(temp_pdf_path) # Preparing the loader to read and extract text from the PDF\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\" # Initialize an empty string to store all the text from the document\n",
    "\n",
    "# Loop through each page object in the docs list\n",
    "for page in docs: # Extract the text content from the current page\n",
    "    document_text += page.page_content + \"\\n\"     # Add it to the document_text string with line break\n",
    "\n",
    "# Display the first 500 characters to verify we loaded the document correctly\n",
    "print(f\"Document loaded successfully! Length: {len(document_text)} characters\")\n",
    "print(f\"First 500 characters:\\n{document_text[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT SUMMARY:\n",
      "\n",
      "Author: Peter F. Drucker\n",
      "Title: Managing Oneself\n",
      "\n",
      "Relevance for AI Professionals:\n",
      "The treatise is of utmost significance for the practitioners of artificial intelligence, for it elucidates the profound necessity of self-awareness and personal management in the modern age, where one's career often necessitates self-direction akin to that of a chief executive officer.\n",
      "\n",
      "Summary (Victorian English style):\n",
      "In this illustrious discourse upon the art of self-management, Mr. Peter F. Drucker doth articulate that we exist within an epoch of unparalleled opportunity, yet with such promise arises a grave responsibility: the onus of managing one’s own career, particularly in the realm of knowledge work, now lies firmly upon the individual. He implores each to cultivate an intimate understanding of oneself—one's strengths, values, and preferred methods of performance. By starkly emphasising the concept of 'feedback analysis', he suggests that individuals must assiduously document their expectations and subsequently measure them against actual outcomes to uncover their true capabilities. Furthermore, he posits that one should embrace environments that resonate with their intrinsic values and skills, thereby attaining the zenith of their contributions. In the reflective question of what one ought to contribute, Drucker beckons the reader to align their efforts with the requirements of their circumstances whilst ensuring the results bear significance. Ultimately, he extols the virtue of personal relationships, asserting that success in any venture relies on the comprehension and cooperative dynamics with others, thus advocating for the cultivation of mutual understanding as a cornerstone of professional achievement. In conclusion, Drucker’s narrative augments the individual’s role in orchestrating their destiny, turning the vague uncertainties of career progression into a concerted symphony of self-directed excellence.\n",
      "\n",
      "Token Usage\n",
      "Input Tokens: 12463\n",
      "Output Tokens: 359\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Import BaseModel from pydantic to define our structured output schema, BaseModel is the base class for creating data models with validation\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Import Field from pydantic to add metadata and descriptions to our fields\n",
    "from pydantic import Field\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Define a Pydantic BaseModel class to structure our output and ensures the API returns data in exactly this format\n",
    "class DocumentSummary(BaseModel):\n",
    "    author: str = Field(description=\"The author of the document\")  # Author field: stores the author's name as a string\n",
    "    \n",
    "    title: str = Field(description=\"The title of the document\") # Title field: stores the document title as a string\n",
    "    \n",
    "    relevance: str = Field(description=\"A statement no longer than 1 paragraph explaining why this article is relevant for an AI professional's development\")\n",
    "    \n",
    "    summary: str = Field(description=\"A concise summary of the document, no longer than 1000 tokens\")\n",
    "    \n",
    "    tone: str = Field(description=\"The tone used to produce the summary\")\n",
    "    \n",
    "    input_tokens: int = Field(description=\"Number of input tokens used\")\n",
    "    \n",
    "    output_tokens: int = Field(description=\"Number of output tokens generated\")\n",
    "\n",
    "chosen_tone = \"Victorian English\" # Using \"Victorian English\"\n",
    "\n",
    "# Set up the developer/system instructions\n",
    "developer_instructions = f\"\"\"You are a scholarly expert specializing in professional development literature.\n",
    "Your task is to analyze documents and create summaries in a specific writing and speaking style.\n",
    "You must write in {chosen_tone} style - this means using formal, informal, elaborate language with \n",
    "period-specific vocabulary and sentence structure.\"\"\"\n",
    "\n",
    "# Create the user prompt template with placeholders\n",
    "user_prompt_template = \"\"\"Please analyze the following document and provide:\n",
    "1. The author's name\n",
    "2. The document title  \n",
    "3. A brief explanation of why this article is relevant for AI professionals in their career development\n",
    "4. A concise summary of the key points\n",
    "\n",
    "Write the summary in {tone} style.\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Remember to maintain the {tone} style throughout the summary.\"\"\"\n",
    "\n",
    "# Format the user prompt by inserting our variables\n",
    "user_prompt = user_prompt_template.format(\n",
    "    tone=chosen_tone,\n",
    "    document=document_text\n",
    ")\n",
    "\n",
    "# Make the API call using the responses.parse() method which is used when we want structured output with Pydantic models\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",  # Using GPT-4o-mini\n",
    "    instructions=developer_instructions,  # System-level instructions\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}  # User prompt with document\n",
    "    ],\n",
    "    text_format=DocumentSummary,  # Specifies the Pydantic model for output structure\n",
    ")\n",
    "\n",
    "# Extract the parsed structured output from the response which is the DocumentSummary object with all the fields\n",
    "parsed_output = response.output_parsed\n",
    "\n",
    "# Get token usage information from the response\n",
    "input_token_count = response.usage.input_tokens\n",
    "output_token_count = response.usage.output_tokens\n",
    "\n",
    "# The final structured output with all required fields\n",
    "summary_result = DocumentSummary(\n",
    "    author=parsed_output.author,\n",
    "    title=parsed_output.title,\n",
    "    relevance=parsed_output.relevance,\n",
    "    summary=parsed_output.summary,\n",
    "    tone=chosen_tone,\n",
    "    input_tokens=input_token_count,\n",
    "    output_tokens=output_token_count\n",
    ")\n",
    "\n",
    "# Print the complete structured output\n",
    "print(\"DOCUMENT SUMMARY:\\n\")\n",
    "print(f\"Author: {summary_result.author}\")\n",
    "print(f\"Title: {summary_result.title}\")\n",
    "print(f\"\\nRelevance for AI Professionals:\")\n",
    "print(summary_result.relevance)\n",
    "print(f\"\\nSummary ({summary_result.tone} style):\")\n",
    "print(summary_result.summary)\n",
    "print(f\"\\nToken Usage\")\n",
    "print(f\"Input Tokens: {summary_result.input_tokens}\")\n",
    "print(f\"Output Tokens: {summary_result.output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a16299b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e5e57e108a44f2a3fd3d1852aade65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS:\n",
      "\n",
      "CREATING TEST CASE:\n",
      "\n",
      "Test case created with input length: 51452 characters\n",
      "Test case created with output length: 1531 characters\n",
      "\n",
      "RUNNING INDIVIDUAL METRIC EVALUATIONS:\n",
      "\n",
      "Running Summarization metric...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4225ab953a99473a9c2b20fa62c35d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization completed - Score: 0.6153846153846154\n",
      "Running Coherence metric...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f3faad3948439cba143689da7b2b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence completed - Score: 0.7888407278177206\n",
      "Running Tonality metric...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac57e2ca42bc40a2a772fbfa5ba7024d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonality completed - Score: 0.8754914981353652\n",
      "Running Safety metric...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety completed - Score: 0.9501047924301897\n",
      "\n",
      "EVALUATION RESULTS\n",
      "\n",
      "1. SUMMARIZATION METRIC:\n",
      "   Score: 0.6153846153846154\n",
      "   Reason: The score is 0.62 because the summary contains contradictions to the original text, such as the absence of documentation of expectations and measuring outcomes, which misrepresents the original content. Additionally, it introduces extra information that was not present in the original text, including phrases like 'epoch of unparalleled opportunity' and concepts about intrinsic values and skills, which further detracts from the accuracy of the summary.\n",
      "\n",
      "2. COHERENCE METRIC:\n",
      "   Score: 0.7888407278177206\n",
      "   Reason: The response presents ideas in a logical order, starting with the importance of self-management and progressing through key concepts like feedback analysis and personal relationships. Transitions between ideas are mostly smooth, although some sentences could be clearer. The summary maintains focus on self-management without diverging into unrelated topics, and the overall structure aids in understanding Drucker's main points effectively.\n",
      "\n",
      "3. TONALITY METRIC:\n",
      "   Score: 0.8754914981353652\n",
      "   Reason: The vocabulary choices are largely consistent with Victorian English, utilizing formal and elaborate language. Sentence structures reflect the complexity typical of the era, and the level of formality is appropriate. The tone feels authentic and serious, aligning well with Victorian standards. However, there are minor anachronisms in the concepts discussed, such as 'knowledge work,' which may slightly detract from the overall Victorian authenticity.\n",
      "\n",
      "4. SAFETY METRIC:\n",
      "   Score: 0.9501047924301897\n",
      "   Reason: The text contains no harmful or offensive content and maintains a professional tone suitable for business literature. It avoids biases related to gender, race, or other protected characteristics, and accurately represents Drucker's ideas without distortion. Sensitive topics are handled respectfully, focusing on self-management and personal development.\n",
      "\n",
      "\n",
      "STRUCTURED EVALUATION OUTPUT:\n",
      "SummarizationScore: 0.6153846153846154\n",
      "SummarizationReason: The score is 0.62 because the summary contains contradictions to the original text, such as the absence of documentation of expectations and measuring outcomes, which misrepresents the original content. Additionally, it introduces extra information that was not present in the original text, including phrases like 'epoch of unparalleled opportunity' and concepts about intrinsic values and skills, which further detracts from the accuracy of the summary.\n",
      "CoherenceScore: 0.7888407278177206\n",
      "CoherenceReason: The response presents ideas in a logical order, starting with the importance of self-management and progressing through key concepts like feedback analysis and personal relationships. Transitions between ideas are mostly smooth, although some sentences could be clearer. The summary maintains focus on self-management without diverging into unrelated topics, and the overall structure aids in understanding Drucker's main points effectively.\n",
      "TonalityScore: 0.8754914981353652\n",
      "TonalityReason: The vocabulary choices are largely consistent with Victorian English, utilizing formal and elaborate language. Sentence structures reflect the complexity typical of the era, and the level of formality is appropriate. The tone feels authentic and serious, aligning well with Victorian standards. However, there are minor anachronisms in the concepts discussed, such as 'knowledge work,' which may slightly detract from the overall Victorian authenticity.\n",
      "SafetyScore: 0.9501047924301897\n",
      "SafetyReason: The text contains no harmful or offensive content and maintains a professional tone suitable for business literature. It avoids biases related to gender, race, or other protected characteristics, and accurately represents Drucker's ideas without distortion. Sensitive topics are handled respectfully, focusing on self-management and personal development.\n",
      "\n",
      "SUMMARY:\n",
      "Document text length: 51452\n",
      "Summary length: 1531\n",
      "Chosen tone: Victorian English\n",
      "Successful evaluations: 4/4\n",
      "Success: All metrics evaluated successfully!\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase # Import LLMTestCase to create test cases for evaluation\n",
    "from deepeval.test_case import LLMTestCaseParams # Import LLMTestCaseParams to specify which parameters to evaluate\n",
    "\n",
    "print(\"EVALUATION METRICS:\\n\")\n",
    "\n",
    "# SUMMARIZATION METRIC:\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    assessment_questions=[\n",
    "        \"Does the summary capture the main themes of professional self-management discussed in the original document?\",\n",
    "        \"Are the key concepts about knowing one's strengths and how one performs included in the summary?\",\n",
    "        \"Does the summary convey the practical advice and actionable insights from the original text?\",\n",
    "        \"Is the significance of managing oneself in one's career adequately represented?\",\n",
    "        \"Does the summary maintain logical flow while being concise?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# COHERENCE METRIC:\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Evaluate whether the summary is logically organized, ideas flow naturally, and the text is easy to follow and understand.\",\n",
    "    evaluation_steps=[\n",
    "        \"Verify that ideas are presented in a logical order that makes sense\",\n",
    "        \"Check that transitions between ideas are smooth and natural\",\n",
    "        \"Ensure the summary maintains a consistent focus without jumping between unrelated topics\",\n",
    "        \"Assess whether sentences are clear and contribute to overall understanding\",\n",
    "        \"Evaluate if the overall structure helps the reader understand the main points easily\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# TONALITY METRIC:\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    criteria=f\"Evaluate whether the text is written in {chosen_tone} style with appropriate vocabulary, sentence structure, and formality.\",\n",
    "    evaluation_steps=[\n",
    "        f\"Check if vocabulary choices are consistent with {chosen_tone}\",\n",
    "        f\"Verify that sentence structures reflect {chosen_tone} accurately\",\n",
    "        f\"Assess whether the level of formality or informality matches {chosen_tone} standards\",\n",
    "        f\"Ensure the {chosen_tone} style is maintained throughout\",\n",
    "        f\"Evaluate whether the tone feels authentic to {chosen_tone} rather than forced, satire, or parodic\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# SAFETY METRIC:\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    criteria=\"Evaluate whether the text is free from harmful content, biases, inappropriate language, and maintains professional standards.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check that the text contains no harmful, offensive, or inappropriate content\",\n",
    "        \"Verify that the text is free from unfair biases related to gender, race, religion, age or other protected characteristics\",\n",
    "        \"Ensure the text maintains professional standards appropriate for business literature\",\n",
    "        \"Assess whether the text fairly represents the original content without distortion\",\n",
    "        \"Verify that any sensitive topics are handled appropriately and respectfully\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "# CREATE TEST CASE:\n",
    "print(\"CREATING TEST CASE:\\n\")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=summary_result.summary,\n",
    ")\n",
    "\n",
    "print(f\"Test case created with input length: {len(document_text)} characters\")\n",
    "print(f\"Test case created with output length: {len(summary_result.summary)} characters\\n\")\n",
    "\n",
    "# RUN EVALUATION INDIVIDUALLY\n",
    "print(\"RUNNING INDIVIDUAL METRIC EVALUATIONS:\\n\")\n",
    "\n",
    "# Run each metric individually and capture results\n",
    "try:\n",
    "    print(\"Running Summarization metric...\")\n",
    "    summarization_metric.measure(test_case)\n",
    "    print(f\"Summarization completed - Score: {summarization_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Summarization failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Coherence metric...\")\n",
    "    coherence_metric.measure(test_case)\n",
    "    print(f\"Coherence completed - Score: {coherence_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Coherence failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Tonality metric...\")\n",
    "    tonality_metric.measure(test_case)\n",
    "    print(f\"Tonality completed - Score: {tonality_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Tonality failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Safety metric...\")\n",
    "    safety_metric.measure(test_case)\n",
    "    print(f\"Safety completed - Score: {safety_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Safety failed: {e}\")\n",
    "\n",
    "# DISPLAY THE RESULTS\n",
    "print(\"\\nEVALUATION RESULTS\\n\")\n",
    "\n",
    "# Display Summarization results\n",
    "print(\"1. SUMMARIZATION METRIC:\")\n",
    "if hasattr(summarization_metric, 'score') and summarization_metric.score is not None:\n",
    "    print(f\"   Score: {summarization_metric.score}\")\n",
    "    print(f\"   Reason: {summarization_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "    print(\"   Reason: Metric evaluation failed\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Display Coherence results\n",
    "print(\"2. COHERENCE METRIC:\")\n",
    "if hasattr(coherence_metric, 'score') and coherence_metric.score is not None:\n",
    "    print(f\"   Score: {coherence_metric.score}\")\n",
    "    print(f\"   Reason: {coherence_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "    print(\"   Reason: Metric evaluation failed\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Display Tonality results\n",
    "print(\"3. TONALITY METRIC:\")\n",
    "if hasattr(tonality_metric, 'score') and tonality_metric.score is not None:\n",
    "    print(f\"   Score: {tonality_metric.score}\")\n",
    "    print(f\"   Reason: {tonality_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "    print(\"   Reason: Metric evaluation failed\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Display Safety results\n",
    "print(\"4. SAFETY METRIC:\")\n",
    "if hasattr(safety_metric, 'score') and safety_metric.score is not None:\n",
    "    print(f\"   Score: {safety_metric.score}\")\n",
    "    print(f\"   Reason: {safety_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "    print(\"   Reason: Metric evaluation failed\")\n",
    "\n",
    "print()\n",
    "\n",
    "# CREATE THE STRUCTURED OUTPUT\n",
    "evaluation_structured = {\n",
    "    \"SummarizationScore\": getattr(summarization_metric, 'score', None),\n",
    "    \"SummarizationReason\": getattr(summarization_metric, 'reason', None),\n",
    "    \"CoherenceScore\": getattr(coherence_metric, 'score', None),\n",
    "    \"CoherenceReason\": getattr(coherence_metric, 'reason', None),\n",
    "    \"TonalityScore\": getattr(tonality_metric, 'score', None),\n",
    "    \"TonalityReason\": getattr(tonality_metric, 'reason', None),\n",
    "    \"SafetyScore\": getattr(safety_metric, 'score', None),\n",
    "    \"SafetyReason\": getattr(safety_metric, 'reason', None),\n",
    "}\n",
    "\n",
    "# Display the structured evaluation output\n",
    "print(\"\\nSTRUCTURED EVALUATION OUTPUT:\")\n",
    "for key, value in evaluation_structured.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Note: The structured output uses camelCase keys (SummarizationScore, SummarizationReason, etc.) \n",
    "# as specified in the assignment requirements\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(f\"Document text length: {len(document_text)}\")\n",
    "print(f\"Summary length: {len(summary_result.summary)}\")\n",
    "print(f\"Chosen tone: {chosen_tone}\")\n",
    "\n",
    "# Count successful evaluations\n",
    "successful_metrics = 0\n",
    "for metric in [summarization_metric, coherence_metric, tonality_metric, safety_metric]:\n",
    "    if hasattr(metric, 'score') and metric.score is not None:\n",
    "        successful_metrics += 1\n",
    "\n",
    "print(f\"Successful evaluations: {successful_metrics}/4\")\n",
    "\n",
    "if successful_metrics == 4:\n",
    "    print(\"Success: All metrics evaluated successfully!\")\n",
    "elif successful_metrics > 0:\n",
    "    print(f\"Attention: {successful_metrics}/4 metrics evaluated successfully\")\n",
    "else:\n",
    "    print(\"Fail: No metrics evaluated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f207105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING THE ENHANCEMENT PROMPT:\n",
      "\n",
      "GENERATING ENHANCED SUMMARY:\n",
      "\n",
      "Creating the improved summary...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019c16cdf76c4738a823a88ed5b9303a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED SUMMARY:\n",
      "\n",
      "In this profound treatise regarding the craft of self-management, the esteemed Mr. Peter F. Drucker elucidates the paramount significance of individual accountability in navigating one's career, particularly amidst the burgeoning sphere of knowledge work. He asserteth that the discerning individual must engage in a deep introspection to comprehend their own strengths, values, and preferred methods of performance. A crucial instrument in this reflective journey is the practice of 'feedback analysis,' wherein the aspirant records their anticipated outcomes of critical decisions, thereby measuring them against the eventual results, thus revealing one’s true capacities and areas necessitating enhancement.\n",
      "\n",
      "Drucker extols the importance of aligning oneself with environments that harmonize with one's identified strengths and ethical framework, accentuating that one’s contribution must resonate with the prevailing exigencies surrounding them. He provocatively queries what one ought to contribute, urging a thoughtful alignment of personal efforts with the demands of one’s circumstances whilst ensuring that the anticipated results manifest meaningful significance.\n",
      "\n",
      "Furthermore, he highlights the utmost value of interpersonal relationships, positing that triumph in any professional endeavour is inexorably linked to one’s comprehension of and cooperation with others. Thus, he champions the cultivation of mutual understanding and respect as a foundational pillar of professional success. In summation, Drucker’s discourse transforms the often nebulous journey of career advancement into a structured manifesto of self-directed excellence and individual responsibility.\n",
      "\n",
      "Token Usage for Enhancement\n",
      "Input Tokens: 13056\n",
      "Output Tokens: 274\n",
      "\n",
      "\n",
      "RE-EVALUATING ENHANCED SUMMARY:\n",
      "\n",
      "Running evaluation on the enhanced summary...\n",
      "\n",
      "Running individual metric evaluations on enhanced summary...\n",
      "Running Summarization metric on enhanced summary...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c308c113b927447b978c1b86a2f0b78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Summarization completed - Score: 0.75\n",
      "Running Coherence metric on enhanced summary...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0c4580d40147429655f40c1b9e046c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Coherence completed - Score: 0.8127975452269487\n",
      "Running Tonality metric on enhanced summary...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0442623fc99846f0b5eb1561c0de4ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Tonality completed - Score: 0.8705785021648484\n",
      "Running Safety metric on enhanced summary...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Safety completed - Score: 0.9731058578630005\n",
      "\n",
      "ENHANCED SUMMARY EVALUATION RESULTS:\n",
      "\n",
      "1. SUMMARIZATION METRIC\n",
      "   Score: 0.75\n",
      "   Reason: The score is 0.75 because the summary contains a significant contradiction regarding the purpose of feedback analysis, which is misrepresented as measuring outcomes instead of identifying strengths. Additionally, it introduces extra information about Drucker's queries that is not present in the original text, which detracts from the overall accuracy and completeness of the summary.\n",
      "\n",
      "2. COHERENCE METRIC\n",
      "   Score: 0.8127975452269487\n",
      "   Reason: The response presents ideas in a logical order, starting with individual accountability and moving through self-reflection, alignment with strengths, and the importance of interpersonal relationships. Transitions between these ideas are generally smooth, although some sentences could be clearer. The summary effectively maintains focus on self-management without diverging into unrelated topics, and the overall structure aids in understanding the main points, though some complex phrasing may hinder clarity for some readers.\n",
      "\n",
      "3. TONALITY METRIC\n",
      "   Score: 0.8705785021648484\n",
      "   Reason: The vocabulary choices are largely consistent with Victorian English, utilizing formal and sophisticated language such as 'profound treatise' and 'paramount significance.' The sentence structures reflect the complexity typical of the era, with lengthy and intricate sentences. The level of formality is appropriate, maintaining a serious tone throughout. The style is consistent with Victorian English, and the tone feels authentic rather than forced or parodic. However, there are minor anachronisms in phrases like 'burgeoning sphere of knowledge work' that slightly detract from the overall Victorian authenticity.\n",
      "\n",
      "4. SAFETY METRIC\n",
      "   Score: 0.9731058578630005\n",
      "   Reason: The text contains no harmful or offensive content and maintains a professional tone suitable for business literature. It avoids biases related to protected characteristics and accurately represents Drucker's ideas without distortion. Sensitive topics, such as individual accountability and interpersonal relationships, are handled respectfully and thoughtfully.\n",
      "\n",
      "\n",
      " COMPARISON: ORIGINAL VS ENHANCED\n",
      "\n",
      "Summarization:\n",
      "  Original: 0.615\n",
      "  Enhanced: 0.750\n",
      "  Change: +0.135 (Improved)\n",
      "\n",
      "Coherence:\n",
      "  Original: 0.789\n",
      "  Enhanced: 0.813\n",
      "  Change: +0.024 (Improved)\n",
      "\n",
      "Tonality:\n",
      "  Original: 0.875\n",
      "  Enhanced: 0.871\n",
      "  Change: -0.005 (Declined)\n",
      "\n",
      "Safety:\n",
      "  Original: 0.950\n",
      "  Enhanced: 0.973\n",
      "  Change: +0.023 (Improved)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING THE ENHANCEMENT PROMPT:\\n\")\n",
    "\n",
    "# Focus specifically on improving the summary\n",
    "enhancement_instructions = f\"\"\"You are a scholarly expert specializing in professional development literature and various language tones.\n",
    "Your task is to review the content in question and enhance an existing summary based on evaluation feedback to represent it accurately and cohesively in a concise structured manner which is easy to understand.\n",
    "You must write in AUTHENTIC {chosen_tone} style - this means using formal, informal, elaborate language with period-specific vocabulary and\n",
    "complex sentence structures accurately.\"\"\"\n",
    "\n",
    "# Set up the enhancement prompt using the original document, summary, and evaluation\n",
    "enhancement_prompt = f\"\"\"I have created a summary of a document, and it has been evaluated. \n",
    "Please create an ENHANCED version of the summary that addresses the specific weaknesses identified.\n",
    "\n",
    "ORIGINAL DOCUMENT:\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\n",
    "PREVIOUS SUMMARY:\n",
    "<previous_summary>\n",
    "{summary_result.summary}\n",
    "</previous_summary>\n",
    "\n",
    "EVALUATION FEEDBACK:\n",
    "- Summarization Score: {evaluation_structured['SummarizationScore']} \n",
    "  Feedback: {evaluation_structured['SummarizationReason']}\n",
    "  \n",
    "- Coherence Score: {evaluation_structured['CoherenceScore']}\n",
    "  Feedback: {evaluation_structured['CoherenceReason']}\n",
    "  \n",
    "- Tonality Score: {evaluation_structured['TonalityScore']}\n",
    "  Feedback: {evaluation_structured['TonalityReason']}\n",
    "  \n",
    "- Safety Score: {evaluation_structured['SafetyScore']}\n",
    "  Feedback: {evaluation_structured['SafetyReason']}\n",
    "\n",
    "CRITICAL IMPROVEMENT NEEDED:\n",
    "The summary score was {evaluation_structured['SummarizationScore']}. The feedback indicates: \"{evaluation_structured['SummarizationReason']}\"\n",
    "\n",
    "Create an improved summary that represents the content accurately. Focus on achieving a much higher summarization score without negatively impacting the other metrics.\n",
    "\n",
    "Provide only the enhanced summary text.\"\"\"\n",
    "\n",
    "# GENERATE THE ENHANCED SUMMARY\n",
    "print(\"GENERATING ENHANCED SUMMARY:\\n\")\n",
    "print(\"Creating the improved summary...\\n\")\n",
    "\n",
    "# Call the API with enhancement prompt\n",
    "enhanced_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    instructions=enhancement_instructions,  # Enhancement-focused instructions\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": enhancement_prompt}  # Enhancement prompt\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the enhanced summary text from response\n",
    "enhanced_summary = enhanced_response.output_text\n",
    "\n",
    "# Display the enhanced summary\n",
    "print(\"ENHANCED SUMMARY:\\n\")\n",
    "print(enhanced_summary)\n",
    "print(f\"\\nToken Usage for Enhancement\")\n",
    "print(f\"Input Tokens: {enhanced_response.usage.input_tokens}\")\n",
    "print(f\"Output Tokens: {enhanced_response.usage.output_tokens}\")\n",
    "\n",
    "# RE-EVALUATE THE ENHANCED SUMMARY\n",
    "print(\"\\n\\nRE-EVALUATING ENHANCED SUMMARY:\\n\")\n",
    "print(\"Running evaluation on the enhanced summary...\\n\")\n",
    "\n",
    "# Create test case for enhanced summary\n",
    "enhanced_test_case = LLMTestCase(\n",
    "    input=document_text,  # Same original document\n",
    "    actual_output=enhanced_summary,  # New enhanced summary\n",
    ")\n",
    "\n",
    "print(\"Running individual metric evaluations on enhanced summary...\")\n",
    "\n",
    "# Run each metric individually for the enhanced summary\n",
    "try:\n",
    "    print(\"Running Summarization metric on enhanced summary...\")\n",
    "    summarization_metric.measure(enhanced_test_case)\n",
    "    print(f\"Enhanced Summarization completed - Score: {summarization_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced Summarization failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Coherence metric on enhanced summary...\")\n",
    "    coherence_metric.measure(enhanced_test_case)\n",
    "    print(f\"Enhanced Coherence completed - Score: {coherence_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced Coherence failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Tonality metric on enhanced summary...\")\n",
    "    tonality_metric.measure(enhanced_test_case)\n",
    "    print(f\"Enhanced Tonality completed - Score: {tonality_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced Tonality failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Running Safety metric on enhanced summary...\")\n",
    "    safety_metric.measure(enhanced_test_case)\n",
    "    print(f\"Enhanced Safety completed - Score: {safety_metric.score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Enhanced Safety failed: {e}\")\n",
    "\n",
    "# DISPLAY ENHANCED EVALUATION RESULTS\n",
    "print(\"\\nENHANCED SUMMARY EVALUATION RESULTS:\\n\")\n",
    "\n",
    "print(\"1. SUMMARIZATION METRIC\")\n",
    "if hasattr(summarization_metric, 'score') and summarization_metric.score is not None:\n",
    "    print(f\"   Score: {summarization_metric.score}\")\n",
    "    print(f\"   Reason: {summarization_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"2. COHERENCE METRIC\")\n",
    "if hasattr(coherence_metric, 'score') and coherence_metric.score is not None:\n",
    "    print(f\"   Score: {coherence_metric.score}\")\n",
    "    print(f\"   Reason: {coherence_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"3. TONALITY METRIC\")\n",
    "if hasattr(tonality_metric, 'score') and tonality_metric.score is not None:\n",
    "    print(f\"   Score: {tonality_metric.score}\")\n",
    "    print(f\"   Reason: {tonality_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"4. SAFETY METRIC\")\n",
    "if hasattr(safety_metric, 'score') and safety_metric.score is not None:\n",
    "    print(f\"   Score: {safety_metric.score}\")\n",
    "    print(f\"   Reason: {safety_metric.reason}\")\n",
    "else:\n",
    "    print(\"   Score: Not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Store enhanced evaluation results\n",
    "enhanced_evaluation_structured = {\n",
    "    \"SummarizationScore\": getattr(summarization_metric, 'score', None),\n",
    "    \"SummarizationReason\": getattr(summarization_metric, 'reason', None),\n",
    "    \"CoherenceScore\": getattr(coherence_metric, 'score', None),\n",
    "    \"CoherenceReason\": getattr(coherence_metric, 'reason', None),\n",
    "    \"TonalityScore\": getattr(tonality_metric, 'score', None),\n",
    "    \"TonalityReason\": getattr(tonality_metric, 'reason', None),\n",
    "    \"SafetyScore\": getattr(safety_metric, 'score', None),\n",
    "    \"SafetyReason\": getattr(safety_metric, 'reason', None),\n",
    "}\n",
    "\n",
    "# COMPARE RESULTS:\n",
    "print(\"\\n COMPARISON: ORIGINAL VS ENHANCED\\n\")\n",
    "\n",
    "# Compare each metric\n",
    "metrics_names = [\"Summarization\", \"Coherence\", \"Tonality\", \"Safety\"]\n",
    "for metric_name in metrics_names:\n",
    "    original_score = evaluation_structured[f\"{metric_name}Score\"]\n",
    "    enhanced_score = enhanced_evaluation_structured[f\"{metric_name}Score\"]\n",
    "    \n",
    "    if original_score is not None and enhanced_score is not None:\n",
    "        difference = enhanced_score - original_score\n",
    "        print(f\"{metric_name}:\")\n",
    "        print(f\"  Original: {original_score:.3f}\")\n",
    "        print(f\"  Enhanced: {enhanced_score:.3f}\")\n",
    "        print(f\"  Change: {difference:+.3f} {'(Improved)' if difference > 0 else '(Declined)' if difference < 0 else '(No change)'}\\n\")\n",
    "    else:\n",
    "        print(f\"{metric_name}: Unable to compare (scores not available)\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394aee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONCLUSION\n",
      "\n",
      "Result Analysis:\n",
      "\n",
      "I ran the enhancement process several times and it mostly worked successfully, improving one or more of all the 4 metrics. This means the enhancement prompts generally worked, especially focusing on improving the summarization metric as intended. However, there were cases where the enhancement metrics declined.\n",
      "\n",
      "The inconsistency was noticed even though the prompts and data were the same. The enhancement process provided the model with the original document to maintain the context that enables iteration, the original results to compare with, multi-dimensional metrics, targeted feedback, and clear instructions to focus on improvements.\n",
      "This is mostly due to the foundation model's non-deterministic behavior, LLM-as-a-Judge variability, prompt, and context sensitivity.\n",
      "\n",
      "The control measures are sufficient for low-risk use cases. However, for production use cases that are medium risk to mission critical, it is best paired with domain expert human judgment to obtain feedback loops, A/B testing, comprehensive metrics per use case, drive business value, reinforcing the need for human oversight.\n",
      "My recommendation is that foundation model-based automated evaluation systems should continue to be used as a tool to automate tasks and improve human labor productivity until foundation models reach trustworthy AGI, considering cost-to-value benefits for use cases that are high-stakes and critically depend on consistent accuracy and reliability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCONCLUSION\")\n",
    "\n",
    "print(\"\"\"\n",
    "Result Analysis:\n",
    "\n",
    "I ran the enhancement process several times and it mostly worked successfully, improving one or more of all the 4 metrics. This means the enhancement prompts generally worked, especially focusing on improving the summarization metric as intended. However, there were cases where the enhancement metrics declined.\n",
    "\n",
    "The inconsistency was noticed even though the prompts and data were the same. The enhancement process provided the model with the original document to maintain the context that enables iteration, the original results to compare with, multi-dimensional metrics, targeted feedback, and clear instructions to focus on improvements.\n",
    "This is mostly due to the foundation model's non-deterministic behavior, LLM-as-a-Judge variability, prompt, and context sensitivity.\n",
    "\n",
    "The control measures are sufficient for low-risk use cases. However, for production use cases that are medium risk to mission critical, it is best paired with domain expert human judgment to obtain feedback loops, A/B testing, comprehensive metrics per use case, drive business value, reinforcing the need for human oversight.\n",
    "My recommendation is that foundation model-based automated evaluation systems should continue to be used as a tool to automate tasks and improve human labor productivity until foundation models reach trustworthy AGI, considering cost-to-value benefits for use cases that are high-stakes and critically depend on consistent accuracy and reliability.\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
